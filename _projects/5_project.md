---
layout: page
title: From Sensing to Reasoning
description: 
img: assets/img/Scientist.png
importance: 1
category: work
---
**This work was conducted during my internship at Lawrence Livermore National Laboratory (LLNL)** and was selected as a **Top Presenter**. It was highlighted by [LLNL Top News](https://www.llnl.gov/article/53246/meet-llnl-three-interns-what-makes-lab-special) and [Purdue ECE Top News](https://engineering.purdue.edu/ECE/News/2025/purdue-ece-phd-student-accelerates-ai-research-at-lawrence-livermore-national-laboratory); see also the [Top Presenter feature](https://www.linkedin.com/posts/akamsali_meet-llnl-three-interns-on-what-makes-the-activity-7361102198816034819-6hth?utm_source=share&utm_medium=member_desktop&rcm=ACoAACkHyFQBjhykYmz1qM-zkRatdenc1d0N-Sg). Work is currently in process to be submitted at Digital Discovery. 
Link to the [Poster](../../assets/pdf/LLNL-POST-2009040-AK-DSSI-2025.pdf)

### Abstract

We evaluate multimodal large language models (LLMs) as protocol-aware “reasoning copilots” for self-driving laboratories (SDLs). Open-source families (e.g., Llama, Granite, Gemma, Hermes, LLaVA) and proprietary GPT models are benchmarked across image-based readiness checks, standard lab tasks, infeasible actions, and adversarial instructions. GPT models lead on perception—accurately detecting transparent vessels and counting objects—but no model exceeds 80% overall accuracy under protocol and safety constraints; in several real-world reasoning scenarios, compact open-source models (2–3B parameters) match or surpass GPT performance. These results reveal persistent gaps in fusing multimodal signals with SOP semantics and in reliable, real-time decision-making. We propose a practical path forward: protocol-aware prompting, rigorous safety stress-tests, action logging, and closed-loop evaluation, positioning LLMs as assistive automators with expert fallbacks—rather than autonomous controllers—to accelerate experimental science safely and effectively.

